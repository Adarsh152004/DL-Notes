{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07bbea3",
   "metadata": {},
   "source": [
    "1. Defining Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38163572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.weights_input_hidden = np.random.randn(\n",
    "            self.input_size, self.hidden_size)\n",
    "        self.weights_hidden_output = np.random.randn(\n",
    "            self.hidden_size, self.output_size)\n",
    "\n",
    "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
    "        self.bias_output = np.zeros((1, self.output_size))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa760a49",
   "metadata": {},
   "source": [
    "2. Defining Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbefbac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(self, X):\n",
    "    self.hidden_activation = np.dot(\n",
    "        X, self.weights_input_hidden) + self.bias_hidden\n",
    "    self.hidden_output = self.sigmoid(self.hidden_activation)\n",
    "\n",
    "    self.output_activation = np.dot(\n",
    "        self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "    self.predicted_output = self.sigmoid(self.output_activation)\n",
    "\n",
    "    return self.predicted_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27219a4",
   "metadata": {},
   "source": [
    "3. Defining Backward Network\n",
    "\n",
    "In Backward pass or Back Propagation the errors between the predicted and actual outputs are computed. The gradients are calculated using the derivative of the sigmoid function and weights and biases are updated accordingly.\n",
    "\n",
    "output_error = y - self.predicted_output: calculates the error at the output layer\n",
    "\n",
    "output_delta = output_error * self.sigmoid_derivative(self.predicted_output): calculates the delta for the output layer\n",
    "\n",
    "hidden_error = np.dot(output_delta, self.weights_hidden_output.T): calculates the error at the hidden layer\n",
    "\n",
    "hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output): calculates the delta for the hidden layer\n",
    "\n",
    "self.weights_hidden_output += np.dot(self.hidden_output.T, output_delta) * learning_rate: updates weights between hidden and output layers\n",
    "\n",
    "self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate: updates weights between input and hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21bc30fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, X, y, learning_rate):\n",
    "    output_error = y - self.predicted_output\n",
    "    output_delta = output_error * \\\n",
    "        self.sigmoid_derivative(self.predicted_output)\n",
    "\n",
    "    hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n",
    "    hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)\n",
    "\n",
    "    self.weights_hidden_output += np.dot(self.hidden_output.T,\n",
    "                                         output_delta) * learning_rate\n",
    "    self.bias_output += np.sum(output_delta, axis=0,\n",
    "                               keepdims=True) * learning_rate\n",
    "    self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate\n",
    "    self.bias_hidden += np.sum(hidden_delta, axis=0,\n",
    "                               keepdims=True) * learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f0d0e",
   "metadata": {},
   "source": [
    "4. Training Network\n",
    "\n",
    "The network is trained over 10,000 epochs using the Back Propagation algorithm with a learning rate of 0.1 progressively reducing the error.\n",
    "\n",
    "output = self.feedforward(X): computes the output for the current inputs\n",
    "\n",
    "self.backward(X, y, learning_rate): updates weights and biases using Back Propagation\n",
    "\n",
    "loss = np.mean(np.square(y - output)): calculates the mean squared error (MSE) loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee13ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, X, y, epochs, learning_rate):\n",
    "    for epoch in range(epochs):\n",
    "        output = self.feedforward(X)\n",
    "        self.backward(X, y, learning_rate)\n",
    "        if epoch % 4000 == 0:\n",
    "            loss = np.mean(np.square(y - output))\n",
    "            print(f\"Epoch {epoch}, Loss:{loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95bdd3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.weights_input_hidden = np.random.randn(\n",
    "            self.input_size, self.hidden_size)\n",
    "        self.weights_hidden_output = np.random.randn(\n",
    "            self.hidden_size, self.output_size)\n",
    "\n",
    "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
    "        self.bias_output = np.zeros((1, self.output_size))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def feedforward(self, X):\n",
    "        self.hidden_activation = np.dot(\n",
    "            X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = self.sigmoid(self.hidden_activation)\n",
    "\n",
    "        self.output_activation = np.dot(\n",
    "            self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.predicted_output = self.sigmoid(self.output_activation)\n",
    "\n",
    "        return self.predicted_output\n",
    "    \n",
    "    def backward(self, X, y, learning_rate):\n",
    "        output_error = y - self.predicted_output\n",
    "        output_delta = output_error * \\\n",
    "            self.sigmoid_derivative(self.predicted_output)\n",
    "\n",
    "        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)\n",
    "\n",
    "        self.weights_hidden_output += np.dot(self.hidden_output.T,\n",
    "                                            output_delta) * learning_rate\n",
    "        self.bias_output += np.sum(output_delta, axis=0,\n",
    "                                keepdims=True) * learning_rate\n",
    "        self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0,\n",
    "                                keepdims=True) * learning_rate\n",
    "        \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.feedforward(X)\n",
    "            self.backward(X, y, learning_rate)\n",
    "            if epoch % 4000 == 0:\n",
    "                loss = np.mean(np.square(y - output))\n",
    "                print(f\"Epoch {epoch}, Loss:{loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee24fee",
   "metadata": {},
   "source": [
    "5. Testing Neural Network\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]): defines the input data\n",
    "\n",
    "y = np.array([[0], [1], [1], [0]]): defines the target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e737ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss:0.27483393610619905\n",
      "Epoch 4000, Loss:0.016910194839150794\n",
      "Epoch 8000, Loss:0.0030585123088599135\n",
      "Predictions after training:\n",
      "[[0.0500748 ]\n",
      " [0.95596815]\n",
      " [0.95454819]\n",
      " [0.04282633]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
    "nn.train(X, y, epochs=10000, learning_rate=0.1)\n",
    "\n",
    "output = nn.feedforward(X)\n",
    "print(\"Predictions after training:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d14113a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
